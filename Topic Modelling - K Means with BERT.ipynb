{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import twint\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import string\n",
    "re.compile('<title>(.*)</title>')\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_1 = pd.read_csv(r'C:\\Users\\14708\\Desktop\\New folder\\tweets_1.csv')\n",
    "phase_2 = pd.read_csv(r'C:\\Users\\14708\\Desktop\\New folder\\tweets_2.csv')\n",
    "phase_3 = pd.read_csv(r'C:\\Users\\14708\\Desktop\\New folder\\tweets_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_1 = phase_1[['tweet']]\n",
    "tweets_2 = phase_2[['tweet']]\n",
    "tweets_3 = phase_3[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-4c07039dd817>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tweet'] = tweets_1['tweet'].astype(str)\n",
      "<ipython-input-4-4c07039dd817>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tweet'] = tweets_1['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
      "<ipython-input-4-4c07039dd817>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:x.lower())\n",
      "<ipython-input-4-4c07039dd817>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
      "<ipython-input-4-4c07039dd817>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>markov chain algorithm to delete this sheet yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brenctzen everytime i post a video it literall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasanshahbaz twitter needs a better sign up al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moscow liga recap     u      player of the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i watch like one mde vid a year and it fucks u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>verifying validating the clinical usefulness o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>today may   pm et sciartexchange facebook lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>follow us on linkedin  httpstcoofvymajsym wear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>asu opened the weartech center a first of its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6699 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     markov chain algorithm to delete this sheet yo...\n",
       "1     brenctzen everytime i post a video it literall...\n",
       "2     hasanshahbaz twitter needs a better sign up al...\n",
       "3      moscow liga recap     u      player of the da...\n",
       "4     i watch like one mde vid a year and it fucks u...\n",
       "...                                                 ...\n",
       "6694  verifying validating the clinical usefulness o...\n",
       "6695  today may   pm et sciartexchange facebook lear...\n",
       "6696  follow us on linkedin  httpstcoofvymajsym wear...\n",
       "6697  asu opened the weartech center a first of its ...\n",
       "6698                                                nan\n",
       "\n",
       "[6699 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_1['tweet'] = tweets_1['tweet'].astype(str)\n",
    "tweets_1['tweet'] = tweets_1['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
    "tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:x.lower())\n",
    "tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
    "tweets_1['tweet'] = tweets_1['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n",
    "tweets_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cff7519b286>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tweet'] = tweets_2['tweet'].astype(str)\n",
      "<ipython-input-5-8cff7519b286>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tweet'] = tweets_2['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
      "<ipython-input-5-8cff7519b286>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:x.lower())\n",
      "<ipython-input-5-8cff7519b286>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
      "<ipython-input-5-8cff7519b286>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geoffcmason greencpa lisaformaine strategema w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a late night one  at least the user im still f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with many federal employees working from home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itox will run over   imo  ai iiot cybersecuri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter is letting people threaten joe biden s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>fahrni windows on the desktop ios on mobile i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>i made a web based particle emitter well a pag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>tour builder is a web based tool for storytell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>pandemic or not technology impacts the way we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6923 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     geoffcmason greencpa lisaformaine strategema w...\n",
       "1     a late night one  at least the user im still f...\n",
       "2     with many federal employees working from home ...\n",
       "3      itox will run over   imo  ai iiot cybersecuri...\n",
       "4     twitter is letting people threaten joe biden s...\n",
       "...                                                 ...\n",
       "6918  fahrni windows on the desktop ios on mobile i ...\n",
       "6919  i made a web based particle emitter well a pag...\n",
       "6920  tour builder is a web based tool for storytell...\n",
       "6921  pandemic or not technology impacts the way we ...\n",
       "6922                                                nan\n",
       "\n",
       "[6923 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_2['tweet'] = tweets_2['tweet'].astype(str)\n",
    "tweets_2['tweet'] = tweets_2['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
    "tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:x.lower())\n",
    "tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
    "tweets_2['tweet'] = tweets_2['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n",
    "tweets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-75104e653128>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tweet'] = tweets_3['tweet'].astype(str)\n",
      "<ipython-input-6-75104e653128>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tweet'] = tweets_3['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
      "<ipython-input-6-75104e653128>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:x.lower())\n",
      "<ipython-input-6-75104e653128>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
      "<ipython-input-6-75104e653128>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hashgraphguide hedera  to this effect we emplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>threddyrex as a dyed in the wool c programmer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linorulli catholicguyshow it s because they su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itssan just to let everyone know that dislikin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i looked up online therapy one time and i m no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>with the smart sensor platform   and the intui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>with covid  completely shifting the education ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>still using paper or spreadsheets for your clu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>sariazout muting newsletters ignoring most pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6147 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     hashgraphguide hedera  to this effect we emplo...\n",
       "1     threddyrex as a dyed in the wool c programmer ...\n",
       "2     linorulli catholicguyshow it s because they su...\n",
       "3     itssan just to let everyone know that dislikin...\n",
       "4     i looked up online therapy one time and i m no...\n",
       "...                                                 ...\n",
       "6142  with the smart sensor platform   and the intui...\n",
       "6143  with covid  completely shifting the education ...\n",
       "6144  still using paper or spreadsheets for your clu...\n",
       "6145  sariazout muting newsletters ignoring most pod...\n",
       "6146                                                nan\n",
       "\n",
       "[6147 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_3['tweet'] = tweets_3['tweet'].astype(str)\n",
    "tweets_3['tweet'] = tweets_3['tweet'].map(lambda x: re.sub('[,()#:\\.!?@/]', '', x))\n",
    "tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:x.lower())\n",
    "tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:re.sub('[0-9\\n]','',x))\n",
    "tweets_3['tweet'] = tweets_3['tweet'].map(lambda x:re.sub( '[^a-z0-9]', ' ', x))\n",
    "tweets_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-9f33a60559d3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['tokenized_tweet'] = tweets_1.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
      "<ipython-input-7-9f33a60559d3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['stopwords_removed'] = tweets_1['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_1])\n",
      "<ipython-input-7-9f33a60559d3>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['punctuations_removed'] = tweets_1['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
      "<ipython-input-7-9f33a60559d3>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1['punctuations_removed'] = tweets_1['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
      "<ipython-input-7-9f33a60559d3>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_1 ['Lemmatized_tweets']=''\n",
      "C:\\Users\\14708\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>punctuations_removed</th>\n",
       "      <th>Lemmatized_tweets</th>\n",
       "      <th>Noun_Extracted_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>markov chain algorithm to delete this sheet yo...</td>\n",
       "      <td>[markov, chain, algorithm, to, delete, this, s...</td>\n",
       "      <td>[markov, chain, algorithm, delete, sheet, may,...</td>\n",
       "      <td>[markov, chain, algorithm, delete, sheet, may,...</td>\n",
       "      <td>[markov, chain, algorithm, delete, sheet, may,...</td>\n",
       "      <td>[markov, chain, sheet, generate, text, generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brenctzen everytime i post a video it literall...</td>\n",
       "      <td>[brenctzen, everytime, i, post, a, video, it, ...</td>\n",
       "      <td>[brenctzen, everytime, post, video, literally,...</td>\n",
       "      <td>[brenctzen, everytime, post, video, literally,...</td>\n",
       "      <td>[brenctzen, everytime, post, video, literally,...</td>\n",
       "      <td>[everytime, post, video, algorithm, lmao]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasanshahbaz twitter needs a better sign up al...</td>\n",
       "      <td>[hasanshahbaz, twitter, needs, a, better, sign...</td>\n",
       "      <td>[hasanshahbaz, twitter, needs, better, sign, a...</td>\n",
       "      <td>[hasanshahbaz, twitter, needs, better, sign, a...</td>\n",
       "      <td>[hasanshahbaz, twitter, need, better, sign, al...</td>\n",
       "      <td>[hasanshahbaz, twitter, sign, ban, people, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moscow liga recap     u      player of the da...</td>\n",
       "      <td>[moscow, liga, recap, u, player, of, the, day,...</td>\n",
       "      <td>[moscow, liga, recap, u, player, day, menshiko...</td>\n",
       "      <td>[moscow, liga, recap, u, player, day, menshiko...</td>\n",
       "      <td>[moscow, liga, recap, u, player, day, menshiko...</td>\n",
       "      <td>[moscow, liga, recap, player, day, mention, sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i watch like one mde vid a year and it fucks u...</td>\n",
       "      <td>[i, watch, like, one, mde, vid, a, year, and, ...</td>\n",
       "      <td>[watch, like, one, mde, vid, year, fucks, yout...</td>\n",
       "      <td>[watch, like, one, mde, vid, year, fucks, yout...</td>\n",
       "      <td>[watch, like, one, mde, vid, year, fuck, youtu...</td>\n",
       "      <td>[watch, mde, year, month]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>verifying validating the clinical usefulness o...</td>\n",
       "      <td>[verifying, validating, the, clinical, usefuln...</td>\n",
       "      <td>[verifying, validating, clinical, usefulness, ...</td>\n",
       "      <td>[verifying, validating, clinical, usefulness, ...</td>\n",
       "      <td>[verifying, validating, clinical, usefulness, ...</td>\n",
       "      <td>[technology, httpstcofypewuizg, theofficialacm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>today may   pm et sciartexchange facebook lear...</td>\n",
       "      <td>[today, may, pm, et, sciartexchange, facebook,...</td>\n",
       "      <td>[today, may, pm, et, sciartexchange, facebook,...</td>\n",
       "      <td>[today, may, pm, et, sciartexchange, facebook,...</td>\n",
       "      <td>[today, may, pm, et, sciartexchange, facebook,...</td>\n",
       "      <td>[today, sciartexchange, facebook, learn, garme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>follow us on linkedin  httpstcoofvymajsym wear...</td>\n",
       "      <td>[follow, us, on, linkedin, httpstcoofvymajsym,...</td>\n",
       "      <td>[follow, us, linkedin, httpstcoofvymajsym, wea...</td>\n",
       "      <td>[follow, us, linkedin, httpstcoofvymajsym, wea...</td>\n",
       "      <td>[follow, u, linkedin, httpstcoofvymajsym, wear...</td>\n",
       "      <td>[linkedin, httpstcoofvymajsym, wearabletech, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>asu opened the weartech center a first of its ...</td>\n",
       "      <td>[asu, opened, the, weartech, center, a, first,...</td>\n",
       "      <td>[asu, opened, weartech, center, first, kind, w...</td>\n",
       "      <td>[asu, opened, weartech, center, first, kind, w...</td>\n",
       "      <td>[asu, opened, weartech, center, first, kind, w...</td>\n",
       "      <td>[asu, center, kind, technology, research, cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6699 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     markov chain algorithm to delete this sheet yo...   \n",
       "1     brenctzen everytime i post a video it literall...   \n",
       "2     hasanshahbaz twitter needs a better sign up al...   \n",
       "3      moscow liga recap     u      player of the da...   \n",
       "4     i watch like one mde vid a year and it fucks u...   \n",
       "...                                                 ...   \n",
       "6694  verifying validating the clinical usefulness o...   \n",
       "6695  today may   pm et sciartexchange facebook lear...   \n",
       "6696  follow us on linkedin  httpstcoofvymajsym wear...   \n",
       "6697  asu opened the weartech center a first of its ...   \n",
       "6698                                                nan   \n",
       "\n",
       "                                        tokenized_tweet  \\\n",
       "0     [markov, chain, algorithm, to, delete, this, s...   \n",
       "1     [brenctzen, everytime, i, post, a, video, it, ...   \n",
       "2     [hasanshahbaz, twitter, needs, a, better, sign...   \n",
       "3     [moscow, liga, recap, u, player, of, the, day,...   \n",
       "4     [i, watch, like, one, mde, vid, a, year, and, ...   \n",
       "...                                                 ...   \n",
       "6694  [verifying, validating, the, clinical, usefuln...   \n",
       "6695  [today, may, pm, et, sciartexchange, facebook,...   \n",
       "6696  [follow, us, on, linkedin, httpstcoofvymajsym,...   \n",
       "6697  [asu, opened, the, weartech, center, a, first,...   \n",
       "6698                                              [nan]   \n",
       "\n",
       "                                      stopwords_removed  \\\n",
       "0     [markov, chain, algorithm, delete, sheet, may,...   \n",
       "1     [brenctzen, everytime, post, video, literally,...   \n",
       "2     [hasanshahbaz, twitter, needs, better, sign, a...   \n",
       "3     [moscow, liga, recap, u, player, day, menshiko...   \n",
       "4     [watch, like, one, mde, vid, year, fucks, yout...   \n",
       "...                                                 ...   \n",
       "6694  [verifying, validating, clinical, usefulness, ...   \n",
       "6695  [today, may, pm, et, sciartexchange, facebook,...   \n",
       "6696  [follow, us, linkedin, httpstcoofvymajsym, wea...   \n",
       "6697  [asu, opened, weartech, center, first, kind, w...   \n",
       "6698                                              [nan]   \n",
       "\n",
       "                                   punctuations_removed  \\\n",
       "0     [markov, chain, algorithm, delete, sheet, may,...   \n",
       "1     [brenctzen, everytime, post, video, literally,...   \n",
       "2     [hasanshahbaz, twitter, needs, better, sign, a...   \n",
       "3     [moscow, liga, recap, u, player, day, menshiko...   \n",
       "4     [watch, like, one, mde, vid, year, fucks, yout...   \n",
       "...                                                 ...   \n",
       "6694  [verifying, validating, clinical, usefulness, ...   \n",
       "6695  [today, may, pm, et, sciartexchange, facebook,...   \n",
       "6696  [follow, us, linkedin, httpstcoofvymajsym, wea...   \n",
       "6697  [asu, opened, weartech, center, first, kind, w...   \n",
       "6698                                              [nan]   \n",
       "\n",
       "                                      Lemmatized_tweets  \\\n",
       "0     [markov, chain, algorithm, delete, sheet, may,...   \n",
       "1     [brenctzen, everytime, post, video, literally,...   \n",
       "2     [hasanshahbaz, twitter, need, better, sign, al...   \n",
       "3     [moscow, liga, recap, u, player, day, menshiko...   \n",
       "4     [watch, like, one, mde, vid, year, fuck, youtu...   \n",
       "...                                                 ...   \n",
       "6694  [verifying, validating, clinical, usefulness, ...   \n",
       "6695  [today, may, pm, et, sciartexchange, facebook,...   \n",
       "6696  [follow, u, linkedin, httpstcoofvymajsym, wear...   \n",
       "6697  [asu, opened, weartech, center, first, kind, w...   \n",
       "6698                                              [nan]   \n",
       "\n",
       "                                  Noun_Extracted_tweets  \n",
       "0     [markov, chain, sheet, generate, text, generat...  \n",
       "1             [everytime, post, video, algorithm, lmao]  \n",
       "2     [hasanshahbaz, twitter, sign, ban, people, acc...  \n",
       "3     [moscow, liga, recap, player, day, mention, sk...  \n",
       "4                             [watch, mde, year, month]  \n",
       "...                                                 ...  \n",
       "6694    [technology, httpstcofypewuizg, theofficialacm]  \n",
       "6695  [today, sciartexchange, facebook, learn, garme...  \n",
       "6696  [linkedin, httpstcoofvymajsym, wearabletech, w...  \n",
       "6697  [asu, center, kind, technology, research, cent...  \n",
       "6698                                              [nan]  \n",
       "\n",
       "[6699 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_1['tokenized_tweet'] = tweets_1.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "stop_1 = set(stopwords.words('english'))\n",
    "tweets_1['stopwords_removed'] = tweets_1['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_1])\n",
    "punc = string.punctuation\n",
    "tweets_1['punctuations_removed'] = tweets_1['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
    "digits = string.digits\n",
    "tweets_1['punctuations_removed'] = tweets_1['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tweets_1 ['Lemmatized_tweets']=''\n",
    "for i in range(0,len(tweets_1)):\n",
    "    tweets_1['Lemmatized_tweets'].iloc[i]=[lemmatizer.lemmatize(x) for x in tweets_1['punctuations_removed'].iloc[i]]\n",
    "nouns = []\n",
    "tweets_1 ['Noun_Extracted_tweets']=''\n",
    "for i in range (len(tweets_1['Lemmatized_tweets'])):\n",
    "    for (word, pos) in nltk.pos_tag(tweets_1['Lemmatized_tweets'].iloc[i]):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "    tweets_1['Noun_Extracted_tweets'].iloc[i] = nouns\n",
    "    nouns = []\n",
    "    \n",
    "tweets_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-5365e2d2b515>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['tokenized_tweet'] = tweets_2.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
      "<ipython-input-8-5365e2d2b515>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['stopwords_removed'] = tweets_2['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_2])\n",
      "<ipython-input-8-5365e2d2b515>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['punctuations_removed'] = tweets_2['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
      "<ipython-input-8-5365e2d2b515>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2['punctuations_removed'] = tweets_2['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
      "<ipython-input-8-5365e2d2b515>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_2 ['Lemmatized_tweets']=''\n",
      "C:\\Users\\14708\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>punctuations_removed</th>\n",
       "      <th>Lemmatized_tweets</th>\n",
       "      <th>Noun_Extracted_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geoffcmason greencpa lisaformaine strategema w...</td>\n",
       "      <td>[geoffcmason, greencpa, lisaformaine, stratege...</td>\n",
       "      <td>[geoffcmason, greencpa, lisaformaine, stratege...</td>\n",
       "      <td>[geoffcmason, greencpa, lisaformaine, stratege...</td>\n",
       "      <td>[geoffcmason, greencpa, lisaformaine, stratege...</td>\n",
       "      <td>[geoffcmason, greencpa, lisaformaine, stratege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a late night one  at least the user im still f...</td>\n",
       "      <td>[a, late, night, one, at, least, the, user, im...</td>\n",
       "      <td>[late, night, one, least, user, im, still, fig...</td>\n",
       "      <td>[late, night, one, least, user, im, still, fig...</td>\n",
       "      <td>[late, night, one, least, user, im, still, fig...</td>\n",
       "      <td>[night, user, im, root, pwn, htb, cybersecurit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with many federal employees working from home ...</td>\n",
       "      <td>[with, many, federal, employees, working, from...</td>\n",
       "      <td>[many, federal, employees, working, home, cybe...</td>\n",
       "      <td>[many, federal, employees, working, home, cybe...</td>\n",
       "      <td>[many, federal, employee, working, home, cyber...</td>\n",
       "      <td>[employee, home, cybersecurity, look, beef, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itox will run over   imo  ai iiot cybersecuri...</td>\n",
       "      <td>[itox, will, run, over, imo, ai, iiot, cyberse...</td>\n",
       "      <td>[itox, run, imo, ai, iiot, cybersecurity, bloc...</td>\n",
       "      <td>[itox, run, imo, ai, iiot, cybersecurity, bloc...</td>\n",
       "      <td>[itox, run, imo, ai, iiot, cybersecurity, bloc...</td>\n",
       "      <td>[itox, imo, ai, iiot, cybersecurity, marketing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter is letting people threaten joe biden s...</td>\n",
       "      <td>[twitter, is, letting, people, threaten, joe, ...</td>\n",
       "      <td>[twitter, letting, people, threaten, joe, bide...</td>\n",
       "      <td>[twitter, letting, people, threaten, joe, bide...</td>\n",
       "      <td>[twitter, letting, people, threaten, joe, bide...</td>\n",
       "      <td>[twitter, people, cybersecurity, expert, smart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>fahrni windows on the desktop ios on mobile i ...</td>\n",
       "      <td>[fahrni, windows, on, the, desktop, ios, on, m...</td>\n",
       "      <td>[fahrni, windows, desktop, ios, mobile, happy,...</td>\n",
       "      <td>[fahrni, windows, desktop, ios, mobile, happy,...</td>\n",
       "      <td>[fahrni, window, desktop, io, mobile, happy, f...</td>\n",
       "      <td>[fahrni, window, desktop, io, mobile, web, agg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>i made a web based particle emitter well a pag...</td>\n",
       "      <td>[i, made, a, web, based, particle, emitter, we...</td>\n",
       "      <td>[made, web, based, particle, emitter, well, pa...</td>\n",
       "      <td>[made, web, based, particle, emitter, well, pa...</td>\n",
       "      <td>[made, web, based, particle, emitter, well, pa...</td>\n",
       "      <td>[web, particle, get, effect, stream, rip, bitr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>tour builder is a web based tool for storytell...</td>\n",
       "      <td>[tour, builder, is, a, web, based, tool, for, ...</td>\n",
       "      <td>[tour, builder, web, based, tool, storytelling...</td>\n",
       "      <td>[tour, builder, web, based, tool, storytelling...</td>\n",
       "      <td>[tour, builder, web, based, tool, storytelling...</td>\n",
       "      <td>[tour, builder, web, tool, text, photo, video,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>pandemic or not technology impacts the way we ...</td>\n",
       "      <td>[pandemic, or, not, technology, impacts, the, ...</td>\n",
       "      <td>[pandemic, technology, impacts, way, interact,...</td>\n",
       "      <td>[pandemic, technology, impacts, way, interact,...</td>\n",
       "      <td>[pandemic, technology, impact, way, interact, ...</td>\n",
       "      <td>[technology, way, interact, business, gpn, opp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6923 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     geoffcmason greencpa lisaformaine strategema w...   \n",
       "1     a late night one  at least the user im still f...   \n",
       "2     with many federal employees working from home ...   \n",
       "3      itox will run over   imo  ai iiot cybersecuri...   \n",
       "4     twitter is letting people threaten joe biden s...   \n",
       "...                                                 ...   \n",
       "6918  fahrni windows on the desktop ios on mobile i ...   \n",
       "6919  i made a web based particle emitter well a pag...   \n",
       "6920  tour builder is a web based tool for storytell...   \n",
       "6921  pandemic or not technology impacts the way we ...   \n",
       "6922                                                nan   \n",
       "\n",
       "                                        tokenized_tweet  \\\n",
       "0     [geoffcmason, greencpa, lisaformaine, stratege...   \n",
       "1     [a, late, night, one, at, least, the, user, im...   \n",
       "2     [with, many, federal, employees, working, from...   \n",
       "3     [itox, will, run, over, imo, ai, iiot, cyberse...   \n",
       "4     [twitter, is, letting, people, threaten, joe, ...   \n",
       "...                                                 ...   \n",
       "6918  [fahrni, windows, on, the, desktop, ios, on, m...   \n",
       "6919  [i, made, a, web, based, particle, emitter, we...   \n",
       "6920  [tour, builder, is, a, web, based, tool, for, ...   \n",
       "6921  [pandemic, or, not, technology, impacts, the, ...   \n",
       "6922                                              [nan]   \n",
       "\n",
       "                                      stopwords_removed  \\\n",
       "0     [geoffcmason, greencpa, lisaformaine, stratege...   \n",
       "1     [late, night, one, least, user, im, still, fig...   \n",
       "2     [many, federal, employees, working, home, cybe...   \n",
       "3     [itox, run, imo, ai, iiot, cybersecurity, bloc...   \n",
       "4     [twitter, letting, people, threaten, joe, bide...   \n",
       "...                                                 ...   \n",
       "6918  [fahrni, windows, desktop, ios, mobile, happy,...   \n",
       "6919  [made, web, based, particle, emitter, well, pa...   \n",
       "6920  [tour, builder, web, based, tool, storytelling...   \n",
       "6921  [pandemic, technology, impacts, way, interact,...   \n",
       "6922                                              [nan]   \n",
       "\n",
       "                                   punctuations_removed  \\\n",
       "0     [geoffcmason, greencpa, lisaformaine, stratege...   \n",
       "1     [late, night, one, least, user, im, still, fig...   \n",
       "2     [many, federal, employees, working, home, cybe...   \n",
       "3     [itox, run, imo, ai, iiot, cybersecurity, bloc...   \n",
       "4     [twitter, letting, people, threaten, joe, bide...   \n",
       "...                                                 ...   \n",
       "6918  [fahrni, windows, desktop, ios, mobile, happy,...   \n",
       "6919  [made, web, based, particle, emitter, well, pa...   \n",
       "6920  [tour, builder, web, based, tool, storytelling...   \n",
       "6921  [pandemic, technology, impacts, way, interact,...   \n",
       "6922                                              [nan]   \n",
       "\n",
       "                                      Lemmatized_tweets  \\\n",
       "0     [geoffcmason, greencpa, lisaformaine, stratege...   \n",
       "1     [late, night, one, least, user, im, still, fig...   \n",
       "2     [many, federal, employee, working, home, cyber...   \n",
       "3     [itox, run, imo, ai, iiot, cybersecurity, bloc...   \n",
       "4     [twitter, letting, people, threaten, joe, bide...   \n",
       "...                                                 ...   \n",
       "6918  [fahrni, window, desktop, io, mobile, happy, f...   \n",
       "6919  [made, web, based, particle, emitter, well, pa...   \n",
       "6920  [tour, builder, web, based, tool, storytelling...   \n",
       "6921  [pandemic, technology, impact, way, interact, ...   \n",
       "6922                                              [nan]   \n",
       "\n",
       "                                  Noun_Extracted_tweets  \n",
       "0     [geoffcmason, greencpa, lisaformaine, stratege...  \n",
       "1     [night, user, im, root, pwn, htb, cybersecurit...  \n",
       "2     [employee, home, cybersecurity, look, beef, de...  \n",
       "3     [itox, imo, ai, iiot, cybersecurity, marketing...  \n",
       "4     [twitter, people, cybersecurity, expert, smart...  \n",
       "...                                                 ...  \n",
       "6918  [fahrni, window, desktop, io, mobile, web, agg...  \n",
       "6919  [web, particle, get, effect, stream, rip, bitr...  \n",
       "6920  [tour, builder, web, tool, text, photo, video,...  \n",
       "6921  [technology, way, interact, business, gpn, opp...  \n",
       "6922                                              [nan]  \n",
       "\n",
       "[6923 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_2['tokenized_tweet'] = tweets_2.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "stop_2 = set(stopwords.words('english'))\n",
    "tweets_2['stopwords_removed'] = tweets_2['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_2])\n",
    "punc = string.punctuation\n",
    "tweets_2['punctuations_removed'] = tweets_2['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
    "digits = string.digits\n",
    "tweets_2['punctuations_removed'] = tweets_2['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tweets_2 ['Lemmatized_tweets']=''\n",
    "for i in range(0,len(tweets_2)):\n",
    "    tweets_2['Lemmatized_tweets'].iloc[i]=[lemmatizer.lemmatize(x) for x in tweets_2['punctuations_removed'].iloc[i]]\n",
    "nouns = []\n",
    "tweets_2 ['Noun_Extracted_tweets']=''\n",
    "for i in range (len(tweets_2['Lemmatized_tweets'])):\n",
    "    for (word, pos) in nltk.pos_tag(tweets_2['Lemmatized_tweets'].iloc[i]):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "    tweets_2['Noun_Extracted_tweets'].iloc[i] = nouns\n",
    "    nouns = []\n",
    "\n",
    "tweets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-7554da8ad8d7>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['tokenized_tweet'] = tweets_3.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
      "<ipython-input-9-7554da8ad8d7>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['stopwords_removed'] = tweets_3['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_3])\n",
      "<ipython-input-9-7554da8ad8d7>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['punctuations_removed'] = tweets_3['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
      "<ipython-input-9-7554da8ad8d7>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3['punctuations_removed'] = tweets_3['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
      "<ipython-input-9-7554da8ad8d7>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_3 ['Lemmatized_tweets']=''\n",
      "C:\\Users\\14708\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>punctuations_removed</th>\n",
       "      <th>Lemmatized_tweets</th>\n",
       "      <th>Noun_Extracted_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hashgraphguide hedera  to this effect we emplo...</td>\n",
       "      <td>[hashgraphguide, hedera, to, this, effect, we,...</td>\n",
       "      <td>[hashgraphguide, hedera, effect, employ, speci...</td>\n",
       "      <td>[hashgraphguide, hedera, effect, employ, speci...</td>\n",
       "      <td>[hashgraphguide, hedera, effect, employ, speci...</td>\n",
       "      <td>[hashgraphguide, hedera, effect, type, paralle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>threddyrex as a dyed in the wool c programmer ...</td>\n",
       "      <td>[threddyrex, as, a, dyed, in, the, wool, c, pr...</td>\n",
       "      <td>[threddyrex, dyed, wool, c, programmer, writte...</td>\n",
       "      <td>[threddyrex, dyed, wool, c, programmer, writte...</td>\n",
       "      <td>[threddyrex, dyed, wool, c, programmer, writte...</td>\n",
       "      <td>[threddyrex, programmer, replacement, algorith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linorulli catholicguyshow it s because they su...</td>\n",
       "      <td>[linorulli, catholicguyshow, it, s, because, t...</td>\n",
       "      <td>[linorulli, catholicguyshow, surveyed, kyle, d...</td>\n",
       "      <td>[linorulli, catholicguyshow, surveyed, kyle, d...</td>\n",
       "      <td>[linorulli, catholicguyshow, surveyed, kyle, d...</td>\n",
       "      <td>[linorulli, catholicguyshow, doug, algorithm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itssan just to let everyone know that dislikin...</td>\n",
       "      <td>[itssan, just, to, let, everyone, know, that, ...</td>\n",
       "      <td>[itssan, let, everyone, know, disliking, video...</td>\n",
       "      <td>[itssan, let, everyone, know, disliking, video...</td>\n",
       "      <td>[itssan, let, everyone, know, disliking, video...</td>\n",
       "      <td>[everyone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i looked up online therapy one time and i m no...</td>\n",
       "      <td>[i, looked, up, online, therapy, one, time, an...</td>\n",
       "      <td>[looked, online, therapy, one, time, sure, ads...</td>\n",
       "      <td>[looked, online, therapy, one, time, sure, ads...</td>\n",
       "      <td>[looked, online, therapy, one, time, sure, ad,...</td>\n",
       "      <td>[online, therapy, time, ad, sign, help, thing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>with the smart sensor platform   and the intui...</td>\n",
       "      <td>[with, the, smart, sensor, platform, and, the,...</td>\n",
       "      <td>[smart, sensor, platform, intuitive, web, base...</td>\n",
       "      <td>[smart, sensor, platform, intuitive, web, base...</td>\n",
       "      <td>[smart, sensor, platform, intuitive, web, base...</td>\n",
       "      <td>[sensor, platform, web, get, capability, phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>with covid  completely shifting the education ...</td>\n",
       "      <td>[with, covid, completely, shifting, the, educa...</td>\n",
       "      <td>[covid, completely, shifting, education, indus...</td>\n",
       "      <td>[covid, completely, shifting, education, indus...</td>\n",
       "      <td>[covid, completely, shifting, education, indus...</td>\n",
       "      <td>[covid, education, industry, format, trend, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>still using paper or spreadsheets for your clu...</td>\n",
       "      <td>[still, using, paper, or, spreadsheets, for, y...</td>\n",
       "      <td>[still, using, paper, spreadsheets, club, char...</td>\n",
       "      <td>[still, using, paper, spreadsheets, club, char...</td>\n",
       "      <td>[still, using, paper, spreadsheet, club, chari...</td>\n",
       "      <td>[paper, spreadsheet, charity, membership, memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>sariazout muting newsletters ignoring most pod...</td>\n",
       "      <td>[sariazout, muting, newsletters, ignoring, mos...</td>\n",
       "      <td>[sariazout, muting, newsletters, ignoring, pod...</td>\n",
       "      <td>[sariazout, muting, newsletters, ignoring, pod...</td>\n",
       "      <td>[sariazout, muting, newsletter, ignoring, podc...</td>\n",
       "      <td>[sariazout, newsletter, podcasts, time, web, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6147 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     hashgraphguide hedera  to this effect we emplo...   \n",
       "1     threddyrex as a dyed in the wool c programmer ...   \n",
       "2     linorulli catholicguyshow it s because they su...   \n",
       "3     itssan just to let everyone know that dislikin...   \n",
       "4     i looked up online therapy one time and i m no...   \n",
       "...                                                 ...   \n",
       "6142  with the smart sensor platform   and the intui...   \n",
       "6143  with covid  completely shifting the education ...   \n",
       "6144  still using paper or spreadsheets for your clu...   \n",
       "6145  sariazout muting newsletters ignoring most pod...   \n",
       "6146                                                nan   \n",
       "\n",
       "                                        tokenized_tweet  \\\n",
       "0     [hashgraphguide, hedera, to, this, effect, we,...   \n",
       "1     [threddyrex, as, a, dyed, in, the, wool, c, pr...   \n",
       "2     [linorulli, catholicguyshow, it, s, because, t...   \n",
       "3     [itssan, just, to, let, everyone, know, that, ...   \n",
       "4     [i, looked, up, online, therapy, one, time, an...   \n",
       "...                                                 ...   \n",
       "6142  [with, the, smart, sensor, platform, and, the,...   \n",
       "6143  [with, covid, completely, shifting, the, educa...   \n",
       "6144  [still, using, paper, or, spreadsheets, for, y...   \n",
       "6145  [sariazout, muting, newsletters, ignoring, mos...   \n",
       "6146                                              [nan]   \n",
       "\n",
       "                                      stopwords_removed  \\\n",
       "0     [hashgraphguide, hedera, effect, employ, speci...   \n",
       "1     [threddyrex, dyed, wool, c, programmer, writte...   \n",
       "2     [linorulli, catholicguyshow, surveyed, kyle, d...   \n",
       "3     [itssan, let, everyone, know, disliking, video...   \n",
       "4     [looked, online, therapy, one, time, sure, ads...   \n",
       "...                                                 ...   \n",
       "6142  [smart, sensor, platform, intuitive, web, base...   \n",
       "6143  [covid, completely, shifting, education, indus...   \n",
       "6144  [still, using, paper, spreadsheets, club, char...   \n",
       "6145  [sariazout, muting, newsletters, ignoring, pod...   \n",
       "6146                                              [nan]   \n",
       "\n",
       "                                   punctuations_removed  \\\n",
       "0     [hashgraphguide, hedera, effect, employ, speci...   \n",
       "1     [threddyrex, dyed, wool, c, programmer, writte...   \n",
       "2     [linorulli, catholicguyshow, surveyed, kyle, d...   \n",
       "3     [itssan, let, everyone, know, disliking, video...   \n",
       "4     [looked, online, therapy, one, time, sure, ads...   \n",
       "...                                                 ...   \n",
       "6142  [smart, sensor, platform, intuitive, web, base...   \n",
       "6143  [covid, completely, shifting, education, indus...   \n",
       "6144  [still, using, paper, spreadsheets, club, char...   \n",
       "6145  [sariazout, muting, newsletters, ignoring, pod...   \n",
       "6146                                              [nan]   \n",
       "\n",
       "                                      Lemmatized_tweets  \\\n",
       "0     [hashgraphguide, hedera, effect, employ, speci...   \n",
       "1     [threddyrex, dyed, wool, c, programmer, writte...   \n",
       "2     [linorulli, catholicguyshow, surveyed, kyle, d...   \n",
       "3     [itssan, let, everyone, know, disliking, video...   \n",
       "4     [looked, online, therapy, one, time, sure, ad,...   \n",
       "...                                                 ...   \n",
       "6142  [smart, sensor, platform, intuitive, web, base...   \n",
       "6143  [covid, completely, shifting, education, indus...   \n",
       "6144  [still, using, paper, spreadsheet, club, chari...   \n",
       "6145  [sariazout, muting, newsletter, ignoring, podc...   \n",
       "6146                                              [nan]   \n",
       "\n",
       "                                  Noun_Extracted_tweets  \n",
       "0     [hashgraphguide, hedera, effect, type, paralle...  \n",
       "1     [threddyrex, programmer, replacement, algorith...  \n",
       "2         [linorulli, catholicguyshow, doug, algorithm]  \n",
       "3                                            [everyone]  \n",
       "4        [online, therapy, time, ad, sign, help, thing]  \n",
       "...                                                 ...  \n",
       "6142  [sensor, platform, web, get, capability, phase...  \n",
       "6143  [covid, education, industry, format, trend, im...  \n",
       "6144  [paper, spreadsheet, charity, membership, memb...  \n",
       "6145  [sariazout, newsletter, podcasts, time, web, l...  \n",
       "6146                                              [nan]  \n",
       "\n",
       "[6147 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_3['tokenized_tweet'] = tweets_3.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "stop_3 = set(stopwords.words('english'))\n",
    "tweets_3['stopwords_removed'] = tweets_3['tokenized_tweet'].apply(lambda y: [item.lower() for item in y if item.lower() not in stop_3])\n",
    "punc = string.punctuation\n",
    "tweets_3['punctuations_removed'] = tweets_3['stopwords_removed'].apply(lambda x: [word for word in x if word not in punc])\n",
    "digits = string.digits\n",
    "tweets_3['punctuations_removed'] = tweets_3['punctuations_removed'].apply(lambda x: [word for word in x if word not in digits])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tweets_3 ['Lemmatized_tweets']=''\n",
    "for i in range(0,len(tweets_3)):\n",
    "    tweets_3['Lemmatized_tweets'].iloc[i]=[lemmatizer.lemmatize(x) for x in tweets_3['punctuations_removed'].iloc[i]]\n",
    "nouns = []\n",
    "tweets_3 ['Noun_Extracted_tweets']=''\n",
    "for i in range (len(tweets_3['Lemmatized_tweets'])):\n",
    "    for (word, pos) in nltk.pos_tag(tweets_3['Lemmatized_tweets'].iloc[i]):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "    tweets_3['Noun_Extracted_tweets'].iloc[i] = nouns\n",
    "    nouns = []   \n",
    "    \n",
    "tweets_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_stopwords1 = {\"help\", \"year\", \"hit\", \"better\", \"week\", \"est\", \"et\", \"way\", \"even\", \"modeling\", \"real\", \"could\", \"well\",\n",
    "\"community\", \"never\", \"great\", \"diagnostic\", \"dans\", \"much\", \"today\", \"many\", \"capacity\", \"analytics\", \"home\", \"feel\", \"thank\", \n",
    "\"high\", \"support\", \"mhealth\", \"httpstcolycabrr\", \"best\", \"rt\", \"management\", \"still\", \"pra\", \"coronavirus\", \"hello\",\"record\",\n",
    "\"le\", \"que\", \"medical\", \"surveillance\", \"el\", \"online\", \"eicu\", \"digital\", \"test\", \"digital\", \"work\", \"good\", \"'s\", \"=01\", \n",
    "\"ect\" , \"''\" , \"``\" , \"subject\" , \"enron\" , \"http\" , \"cc\", \"=20\" , \"e\" , \"risk\", \"improve\", \"learn\", \"flow\", \"going\", \"algorithm\",\n",
    "\"3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\", \"mais\", \"think\", \"mohealth\", \"public\",\n",
    "\"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d\", \"10\" , \"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\",\n",
    "\"'ll\" , \"ees\" , \"'d\" , \"dbcaps97data\" , \"'m\" , \"2/2/2002\" , \"nd=0604\" , \"j\" , \"fw\", \"pandemic\", \"check\",\n",
    "\"st\" , \"06/30/2000\" , \"2/1/2002\" , \"fri\" , \"ct\" , \"l\" , \"'ve\" , \"ena\" , \"inc.\" , \"pt\",\n",
    "\"...\" , \"=09\" , \"imceanotes-erin+20richardson+20+3cerichardson+40sarofim+2ecom+3e+40e\" ,\n",
    "\"steffes/na/enron\" , \"steffes/na/enron\" , \"st.\" , \"eb\" , \"pm\" , \"n't\" , \"2a04\" , \"2001\" , \"enron.com\"\n",
    "'delete','sheet','may','want','//t.co/v46WvaClFC','Te_Papa','desbfit','abc13houston','Fitbit','//t.co/6HosrKWVRa','WHEW','carmodel','feminists','MnDPS_MSP','CAEHomelessness',\n",
    "'//t.co/mBmBkmJaiR','//t.co/bruuMFwcOD','Bij','JamesWallmeup','دعم','//t.co/YtcnYfoUyL','iBox','payer','DoingMore4OC','NAF','NCAA','.com','WHODjibouti',\n",
    "'//t.co/qMjvuQffuM','1_brainyyy','⭕️Take','//t.co/aaF71PHv3X','8500','yoyoel','ノ','hoshinoona','aliens','EPAL','//t.co/SCNEnb6iV7','Une','//t.co/6DRmyhYSOg',\n",
    "'301-600-1506.','MrDtAFC','//t.co/zCZug3MQlN','//t.co/dKbxVHPsik','06.39','Col·lapse','mind…','//t.co/FjWCPRLiXa','//t.co/i21YoKva5I',\"'jigsaw'\",'//t.co/65JUgEhRQE',\n",
    "'//t.co/TZpKoWb25H','10/2','فدوة','//t.co/yUgFHc2dXT','//t.co/B0i8Bag6QI','//t.co/giv5nWtR7s','//t.co/QIrKc2CON5','//t.co/SM9W2a0jg1','pts','//t.co/4r9MKbEFZ8',\n",
    "'//t.co/kCnhzB8EXK','//t.co/SkywBu1lCG','//t.co/N5SZsMSYdS','2020-06-28','793,411.67','//t.co/4o9CkPHo8d','//t.co/PgtfS1u8Ts','LD','//t.co/f1y1cOT4JL',\n",
    "'//t.co/BBLT79cP92','bon','//t.co/BBLT79cP92','//t.co/m6ymYVBwNa','app/browser','del','12/13','-via','عبر','op','الخاص','التابعة','slo','2-year','33'\n",
    "'الفطر','contaminated🤦\\u200d♀️','//t.co/uhNTa7tiWE','call...','الحبل','witch','//t.co/iIKMzII3hp','//t.co/ZFdyj2RNXV','تجهز','CHOOZ','لهرمون','XL','tabib1online',\n",
    "'lomuntu','🙏🏻','//t.co/FpK7srDG6L','GPUs','//t.co/wGr8FARDnt','HIMSSAP','//t.co/eQWmOQRFHx','nya','//t.co/1FVMHg0iH4','11:30:02','5.12','_WWSD_','KIAA1217',\n",
    "'//t.co/yBHgAHuaAZ','//t.co/MbPJnN7jeM','//t.co/WreWejDRta','gt','//t.co/qpxi4Wvtzi','IHadToSaySmtg','食べたい','PoS','31st','vi','//t.co/4rKRU0DPJ4',\n",
    "'الدين','이','فيروس_كورونا','intl','F_D_KB','PushpaShivaram','//t.co/OAiYqcY3jS','wphillips49','Osborn','👏👏','escritório','ICECcancer',\"''\",'تكنيكال',\n",
    "'//t.co/5w7NoVS5d9','//t.co/WuuKFtqM2b','CH4','//t.co/gfHMy7DOGm','CHEX_Org','//t.co/JZB6Srstfl','//t.co/ceJo84uTbE','//t.co/5yqJFchSj3','JosephCosgrove7',\n",
    "'//t.co/8Ud5VOPxih','🙃😂','Nibiru024','46H','//t.co/KVeWbxB5Tz','HR2339','Frølund','MWA/MIWC','773-3760','maybe','إصابة','//t.co/RxeIJvWpZm',\"'quiet\",\n",
    "'//t.co/MNEwX41VcT','//t.co/jtjSbUuUdC','leelo','//t.co/tATJhcT8Wa','podrás','mc93823939','cbs','toptrader101','OnQuorum','📸','//t.co/cuXatP3cft','//t.co/ALepWlrIp7','doodly_official…',\n",
    "'ano','›','الشكر','PHCA','//t.co/gAXaNJZokQ','IN14-21DAYS.HE','//t.co/zqfk8IBNrs','4/28','//t.co/5ntCB6YpNh','tipo','//t.co/GmZarlv7Bw','CHI','//t.co/EAv5ShTtJ5','//t.co/9LGAqM0ePB',\n",
    "'DianaAtwine','FinTech…','LK-05','Maths','//t.co/lR4NTT6ROc','//t.co/svrDN8Fj1e','CSTE','セブン','//t.co/Pl8JYGVZne','1.17','台湾、マスク購入を実名制に','الانبار',\n",
    "'//t.co/DCTC53Z5wm','//t.co/J0px5qKbFe','ABA','//t.co/QymBvyioCe','//t.co/eYTuhbMUkg','//t.co/HWR6SW0ebL','//t.co/zC8C0FXFPQ','//t.co/e8lSxmERyZ','المختبرية',\n",
    "'2/Any','//t.co/jbbsc2A3LH','//t.co/xInOfPUmgr','//t.co/bmm28imSVT','👨\\u200d💻','748-0637','//t.co/jroE1Nr8Ny','on.','n','Spotify','🦕','//t.co/Z34HFPVSDv',\n",
    "'//t.co/bHHS531LhP','今日から職場は全員マスク着用で業務に当たってくださいとのこと。','inane','//t.co/PIMVvPoQfz','//t.co/W0I6OBLRf4',\"y'all\",'العراق','كل','V-Synth','//t.co/lEBfulItxc',\n",
    "'تعلن','//t.co/nK2eTJbZtv','//t.co/w2H0b2pJb8','//t.co/zHa1MDZO93','//t.co/4MBbM5oZhH','sarà','//t.co/N4a3MGBDlX','nicky_2901','//t.co/tnlNG7cvvW','11am',\n",
    "'//t.co/WFjCD7ZEDd','//t.co/TWaBhu9mR4','1,211','//t.co/auB0pVeG7d','//t.co/IdwQ27DdpF','педагог','West','//t.co/YK0qfYUa4R','lorenzozaffiri',\n",
    "\"'leak\",'MOBILE】奥さん、子供たちは休校だってね・・じゃぁ奥さんも休家事しちゃいなよ朝活','29/06/2020','//t.co/SClSErn2cj','//t.co/jEz3uc50B2','//t.co/642K4QjWec','psb_dc','//t.co/6DkDQapDkn',\n",
    "'//t.co/JcFbvMQuNd','SIG','في','محافظة','Phx','dr_ksa12','1:00','//t.co/VecEZfbLma',\"'event\",'//t.co/qaZ2Zw31jE','//t.co/GVCAyNwcTM','//t.co/xgEEtWzIo4',\n",
    "'💜🌺🌼🌻🌸','OBD','ユニリーバやコカ・コーラ、スターバックスなどが続々と、大手SNSのヘイトスピーチ対策が不十分だとして広告掲載を当面見合わせ','تماس','OzCybers','seu','+1','//t.co/03kbvwgYSz','//t.co/e8Etr9nFSw',\n",
    "'//t.co/nRjwRSsmSE','//t.co/LwLqTB0XBI','ny','Ltd','7GTech','18.83','//t.co/rQZphVCP9i','//t.co/AzVDIvUhUE','VS''kwts','//t.co/oYSJtD2Enb','Steph','VideoGamesPlus_','Ayjchan','//t.co/O9htYedxB7',\n",
    "'10.13','//t.co/ylKXRpYeg4','//t.co/pYnxS9x4a3','//t.co/wAr5TTayEC','//t.co/uZHhHbQ47W','//t.co/zoNpVPXV0a','//t.co/RrOiQza4Nm','//t.co/GidfgfcXKn','🗓️Deadline',\n",
    "'00:55:38','.As','KarimZeribi','...','//t.co/hta6isohwv','go',\"https\", \"’\", \"'s\", \"''\", \"``\", \"amp\", \"n't\", \"”\", \"“\", \"de\", \"gt\", \"2020\", \"'re\", \"--\", \"IoT\", \"'m\", \"..\", \n",
    "\"–\", \"en\", \"...\", \"‘\",'health','amp','http','u','one','get','see','using','know','thing','triage','like','first','self-triage','find','would',\n",
    "'la','https',\"n't\",'vid','’','u','fucks','us','use','get','one','de','2020',\"”\",\"‘\",'see','\"','`','health','amp','data','technology',\n",
    "  'ai','doc','blockchain','care','telehealth','need','monitoring','hospital','people','fuck','healthcare','mobile',\n",
    "'visit','medicare','via','covid','information','cost','service','telemedicine','say','new','june','must','patient','stay','iot','follow','wearable','digitalhealth',\n",
    "'take','medium','link','call','sign','access','question','cover','healthtech','make','form','yes','free','dont','ml','self','unit',\n",
    "'th','based','web','tool','mental','member','imaging','website','list','update','page','minute','social','without','change','ehr','system','virtual','situation','contact','also','time','day','case','app','electronic','tracing'}\n",
    "\n",
    "stop_1|= add_stopwords1\n",
    "len(stop_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_stopwords2 = {\"jo\", \"el\", \"record\", \"never\", \"lia\", \"real\", \"online\", \"medical\", \"remote\", \"eicu\", \"'s\", \"=01\", \"ect\" ,\n",
    "\"erro\", \"digital\", \"timebig\", \"melo\", \"exchange\", \"back\", \"cybersecurity\", \"stop\", \"sebasti\", \"bad\", \"interoperability\", \"community\",\n",
    "\"increased\", \"psb\", \"doctor\", \"porto\", \"best\",\n",
    "\"''\" , \"``\" , \"subject\" , \"enron\" , \"http\" , \"cc\", \"=20\" , \"e\", \"help\", \"risk\", \"le\", \"year\", \"life\", \"que\", \"surveillance\", \"still\",\n",
    "\"big\", \"improve\", \"work\", \"even\", \"mar\", \"learn\", \"hit\", \"campos\", \"today\", \"good\", \"really\", \"rt\", \"arraes\", \"think\", \"better\",\n",
    "\"3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\", \"httpstcolycabrr\", \"issue\", \"many\", \"recife\", \n",
    "\"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d\", \"10\" , \"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\",\n",
    "\"'ll\" , \"ees\" , \"'d\" , \"dbcaps97data\" , \"'m\" , \"2/2/2002\" , \"nd=0604\" , \"j\" , \"fw\", \"turno\", \"way\", \"well\", \"high\", \"much\",\n",
    "\"st\" , \"06/30/2000\" , \"2/1/2002\" , \"fri\" , \"ct\" , \"l\" , \"'ve\" , \"ena\" , \"inc.\" , \"pt\", \"votos\", \"test\", \"way\", \"testing\", \"lido\",\n",
    "\"...\" , \"=09\" , \"imceanotes-erin+20richardson+20+3cerichardson+40sarofim+2ecom+3e+40e\" , \"could\", \"going\", \"manuela\", \"well\",\n",
    "\"steffes/na/enron\" , \"steffes/na/enron\" , \"st.\" , \"eb\" , \"pm\" , \"n't\" , \"2a04\" , \"2001\" , \"enron.com\", \"margem\",\n",
    "'delete','sheet','may','want','//t.co/v46WvaClFC','Te_Papa','desbfit','abc13houston','Fitbit','//t.co/6HosrKWVRa','WHEW','carmodel','feminists','MnDPS_MSP','CAEHomelessness',\n",
    "'//t.co/mBmBkmJaiR','//t.co/bruuMFwcOD','Bij','JamesWallmeup','دعم','//t.co/YtcnYfoUyL','iBox','payer','DoingMore4OC','NAF','NCAA','.com','WHODjibouti',\n",
    "'//t.co/qMjvuQffuM','1_brainyyy','⭕️Take','//t.co/aaF71PHv3X','8500','yoyoel','ノ','hoshinoona','aliens','EPAL','//t.co/SCNEnb6iV7','Une','//t.co/6DRmyhYSOg',\n",
    "'301-600-1506.','MrDtAFC','//t.co/zCZug3MQlN','//t.co/dKbxVHPsik','06.39','Col·lapse','mind…','//t.co/FjWCPRLiXa','//t.co/i21YoKva5I',\"'jigsaw'\",'//t.co/65JUgEhRQE',\n",
    "'//t.co/TZpKoWb25H','10/2','فدوة','//t.co/yUgFHc2dXT','//t.co/B0i8Bag6QI','//t.co/giv5nWtR7s','//t.co/QIrKc2CON5','//t.co/SM9W2a0jg1','pts','//t.co/4r9MKbEFZ8',\n",
    "'//t.co/kCnhzB8EXK','//t.co/SkywBu1lCG','//t.co/N5SZsMSYdS','2020-06-28','793,411.67','//t.co/4o9CkPHo8d','//t.co/PgtfS1u8Ts','LD','//t.co/f1y1cOT4JL',\n",
    "'//t.co/BBLT79cP92','bon','//t.co/BBLT79cP92','//t.co/m6ymYVBwNa','app/browser','del','12/13','-via','عبر','op','الخاص','التابعة','slo','2-year','33'\n",
    "'الفطر','contaminated🤦\\u200d♀️','//t.co/uhNTa7tiWE','call...','الحبل','witch','//t.co/iIKMzII3hp','//t.co/ZFdyj2RNXV','تجهز','CHOOZ','لهرمون','XL','tabib1online',\n",
    "'lomuntu','🙏🏻','//t.co/FpK7srDG6L','GPUs','//t.co/wGr8FARDnt','HIMSSAP','//t.co/eQWmOQRFHx','nya','//t.co/1FVMHg0iH4','11:30:02','5.12','_WWSD_','KIAA1217',\n",
    "'//t.co/yBHgAHuaAZ','//t.co/MbPJnN7jeM','//t.co/WreWejDRta','gt','//t.co/qpxi4Wvtzi','IHadToSaySmtg','食べたい','PoS','31st','vi','//t.co/4rKRU0DPJ4',\n",
    "'الدين','이','فيروس_كورونا','intl','F_D_KB','PushpaShivaram','//t.co/OAiYqcY3jS','wphillips49','Osborn','👏👏','escritório','ICECcancer',\"''\",'تكنيكال',\n",
    "'//t.co/5w7NoVS5d9','//t.co/WuuKFtqM2b','CH4','//t.co/gfHMy7DOGm','CHEX_Org','//t.co/JZB6Srstfl','//t.co/ceJo84uTbE','//t.co/5yqJFchSj3','JosephCosgrove7',\n",
    "'//t.co/8Ud5VOPxih','🙃😂','Nibiru024','46H','//t.co/KVeWbxB5Tz','HR2339','Frølund','MWA/MIWC','773-3760','maybe','إصابة','//t.co/RxeIJvWpZm',\"'quiet\",\n",
    "'//t.co/MNEwX41VcT','//t.co/jtjSbUuUdC','leelo','//t.co/tATJhcT8Wa','podrás','mc93823939','cbs','toptrader101','OnQuorum','📸','//t.co/cuXatP3cft','//t.co/ALepWlrIp7','doodly_official…',\n",
    "'ano','›','الشكر','PHCA','//t.co/gAXaNJZokQ','IN14-21DAYS.HE','//t.co/zqfk8IBNrs','4/28','//t.co/5ntCB6YpNh','tipo','//t.co/GmZarlv7Bw','CHI','//t.co/EAv5ShTtJ5','//t.co/9LGAqM0ePB',\n",
    "'DianaAtwine','FinTech…','LK-05','Maths','//t.co/lR4NTT6ROc','//t.co/svrDN8Fj1e','CSTE','セブン','//t.co/Pl8JYGVZne','1.17','台湾、マスク購入を実名制に','الانبار',\n",
    "'//t.co/DCTC53Z5wm','//t.co/J0px5qKbFe','ABA','//t.co/QymBvyioCe','//t.co/eYTuhbMUkg','//t.co/HWR6SW0ebL','//t.co/zC8C0FXFPQ','//t.co/e8lSxmERyZ','المختبرية',\n",
    "'2/Any','//t.co/jbbsc2A3LH','//t.co/xInOfPUmgr','//t.co/bmm28imSVT','👨\\u200d💻','748-0637','//t.co/jroE1Nr8Ny','on.','n','Spotify','🦕','//t.co/Z34HFPVSDv',\n",
    "'//t.co/bHHS531LhP','今日から職場は全員マスク着用で業務に当たってくださいとのこと。','inane','//t.co/PIMVvPoQfz','//t.co/W0I6OBLRf4',\"y'all\",'العراق','كل','V-Synth','//t.co/lEBfulItxc',\n",
    "'تعلن','//t.co/nK2eTJbZtv','//t.co/w2H0b2pJb8','//t.co/zHa1MDZO93','//t.co/4MBbM5oZhH','sarà','//t.co/N4a3MGBDlX','nicky_2901','//t.co/tnlNG7cvvW','11am',\n",
    "'//t.co/WFjCD7ZEDd','//t.co/TWaBhu9mR4','1,211','//t.co/auB0pVeG7d','//t.co/IdwQ27DdpF','педагог','West','//t.co/YK0qfYUa4R','lorenzozaffiri',\n",
    "\"'leak\",'MOBILE】奥さん、子供たちは休校だってね・・じゃぁ奥さんも休家事しちゃいなよ朝活','29/06/2020','//t.co/SClSErn2cj','//t.co/jEz3uc50B2','//t.co/642K4QjWec','psb_dc','//t.co/6DkDQapDkn',\n",
    "'//t.co/JcFbvMQuNd','SIG','في','محافظة','Phx','dr_ksa12','1:00','//t.co/VecEZfbLma',\"'event\",'//t.co/qaZ2Zw31jE','//t.co/GVCAyNwcTM','//t.co/xgEEtWzIo4',\n",
    "'💜🌺🌼🌻🌸','OBD','ユニリーバやコカ・コーラ、スターバックスなどが続々と、大手SNSのヘイトスピーチ対策が不十分だとして広告掲載を当面見合わせ','تماس','OzCybers','seu','+1','//t.co/03kbvwgYSz','//t.co/e8Etr9nFSw',\n",
    "'//t.co/nRjwRSsmSE','//t.co/LwLqTB0XBI','ny','Ltd','7GTech','18.83','//t.co/rQZphVCP9i','//t.co/AzVDIvUhUE','VS''kwts','//t.co/oYSJtD2Enb','Steph','VideoGamesPlus_','Ayjchan','//t.co/O9htYedxB7',\n",
    "'10.13','//t.co/ylKXRpYeg4','//t.co/pYnxS9x4a3','//t.co/wAr5TTayEC','//t.co/uZHhHbQ47W','//t.co/zoNpVPXV0a','//t.co/RrOiQza4Nm','//t.co/GidfgfcXKn','🗓️Deadline',\n",
    "'00:55:38','.As','KarimZeribi','...','//t.co/hta6isohwv','go',\"https\", \"’\", \"'s\", \"''\", \"``\", \"amp\", \"n't\", \"”\", \"“\", \"de\", \"gt\", \"2020\", \"'re\", \"--\", \"IoT\", \"'m\", \"..\", \n",
    "\"–\", \"en\", \"...\", \"‘\",'health','amp','http','u','one','get','see','using','know','thing','triage','like','first','self-triage','find','would',\n",
    "'la','https',\"n't\",'vid','’','u','fucks','us','use','get','one','de','2020',\"”\",\"‘\",'see','\"','`','health','amp','data','technology',\n",
    "  'ai','doc','blockchain','care','telehealth','need','monitoring','hospital','people','fuck','healthcare','mobile',\n",
    "'visit','medicare','via','covid','information','cost','service','telemedicine','say','new','june','must','patient','stay','iot','follow','wearable','digitalhealth',\n",
    "'take','medium','link','call','sign','access','question','cover','healthtech','make','form','yes','free','dont','ml','self','unit',\n",
    "'th','based','web','tool','mental','member','imaging','website','list','update','page','minute','social','without','change','ehr','system','virtual','situation','contact','also','time','day','case','app','electronic','tracing'}\n",
    "\n",
    "stop_2|= add_stopwords2\n",
    "len(stop_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_stopwords3 = {\"medical\", \"hit\", \"surveillance\", \"remote\", \"support\", \"learn\", \"'s\", \"=01\", \"ect\" , \"''\" , \"``\" , \"subject\" , \n",
    "\"enron\" , \"http\" , \"cc\", \"=20\" , \"e\", \"eicu\", \"help\", \"year\", \"le\", \"real\", \"better\", \"week\", \"que\", \"great\", \"think\", \"work\",\n",
    "\"3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\", \"today\", \"et\", \"way\", \"month\", \"est\", \"well\", \"never\"\n",
    "\"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d\", \"10\" , \"=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=3d=\",\n",
    "\"'ll\" , \"ees\" , \"'d\" , \"dbcaps97data\" , \"'m\" , \"2/2/2002\" , \"nd=0604\" , \"j\" , \"fw\", \"el\", \"next\", \"ne\", \"pa\", \"last\",\n",
    "\"st\" , \"06/30/2000\" , \"2/1/2002\" , \"fri\" , \"ct\" , \"l\" , \"'ve\" , \"ena\" , \"inc.\" , \"pt\", \"capacity\", \"improve\", \"algorithm\", \n",
    "\"...\" , \"=09\" , \"imceanotes-erin+20richardson+20+3cerichardson+40sarofim+2ecom+3e+40e\" , \"good\", \"never\", \"join\", \"team\", \"si\",\n",
    "\"steffes/na/enron\" , \"steffes/na/enron\" , \"st.\" , \"eb\" , \"pm\" , \"n't\" , \"2a04\" , \"2001\" , \"enron.com\", \"love\", \"rafac\", \"best\",\n",
    "\"se\", \"even\", \"many\", \"still\",\"sensor\", \"really\", \"pr\", \"could\", \"part\", \"une\", \"back\", \"oh\",\n",
    "'delete','sheet','may','want','//t.co/v46WvaClFC','Te_Papa','desbfit','abc13houston','Fitbit','//t.co/6HosrKWVRa','WHEW','carmodel','feminists','MnDPS_MSP','CAEHomelessness',\n",
    "'//t.co/mBmBkmJaiR','//t.co/bruuMFwcOD','Bij','JamesWallmeup','دعم','//t.co/YtcnYfoUyL','iBox','payer','DoingMore4OC','NAF','NCAA','.com','WHODjibouti',\n",
    "'//t.co/qMjvuQffuM','1_brainyyy','⭕️Take','//t.co/aaF71PHv3X','8500','yoyoel','ノ','hoshinoona','aliens','EPAL','//t.co/SCNEnb6iV7','Une','//t.co/6DRmyhYSOg',\n",
    "'301-600-1506.','MrDtAFC','//t.co/zCZug3MQlN','//t.co/dKbxVHPsik','06.39','Col·lapse','mind…','//t.co/FjWCPRLiXa','//t.co/i21YoKva5I',\"'jigsaw'\",'//t.co/65JUgEhRQE',\n",
    "'//t.co/TZpKoWb25H','10/2','فدوة','//t.co/yUgFHc2dXT','//t.co/B0i8Bag6QI','//t.co/giv5nWtR7s','//t.co/QIrKc2CON5','//t.co/SM9W2a0jg1','pts','//t.co/4r9MKbEFZ8',\n",
    "'//t.co/kCnhzB8EXK','//t.co/SkywBu1lCG','//t.co/N5SZsMSYdS','2020-06-28','793,411.67','//t.co/4o9CkPHo8d','//t.co/PgtfS1u8Ts','LD','//t.co/f1y1cOT4JL',\n",
    "'//t.co/BBLT79cP92','bon','//t.co/BBLT79cP92','//t.co/m6ymYVBwNa','app/browser','del','12/13','-via','عبر','op','الخاص','التابعة','slo','2-year','33'\n",
    "'الفطر','contaminated🤦\\u200d♀️','//t.co/uhNTa7tiWE','call...','الحبل','witch','//t.co/iIKMzII3hp','//t.co/ZFdyj2RNXV','تجهز','CHOOZ','لهرمون','XL','tabib1online',\n",
    "'lomuntu','🙏🏻','//t.co/FpK7srDG6L','GPUs','//t.co/wGr8FARDnt','HIMSSAP','//t.co/eQWmOQRFHx','nya','//t.co/1FVMHg0iH4','11:30:02','5.12','_WWSD_','KIAA1217',\n",
    "'//t.co/yBHgAHuaAZ','//t.co/MbPJnN7jeM','//t.co/WreWejDRta','gt','//t.co/qpxi4Wvtzi','IHadToSaySmtg','食べたい','PoS','31st','vi','//t.co/4rKRU0DPJ4',\n",
    "'الدين','이','فيروس_كورونا','intl','F_D_KB','PushpaShivaram','//t.co/OAiYqcY3jS','wphillips49','Osborn','👏👏','escritório','ICECcancer',\"''\",'تكنيكال',\n",
    "'//t.co/5w7NoVS5d9','//t.co/WuuKFtqM2b','CH4','//t.co/gfHMy7DOGm','CHEX_Org','//t.co/JZB6Srstfl','//t.co/ceJo84uTbE','//t.co/5yqJFchSj3','JosephCosgrove7',\n",
    "'//t.co/8Ud5VOPxih','🙃😂','Nibiru024','46H','//t.co/KVeWbxB5Tz','HR2339','Frølund','MWA/MIWC','773-3760','maybe','إصابة','//t.co/RxeIJvWpZm',\"'quiet\",\n",
    "'//t.co/MNEwX41VcT','//t.co/jtjSbUuUdC','leelo','//t.co/tATJhcT8Wa','podrás','mc93823939','cbs','toptrader101','OnQuorum','📸','//t.co/cuXatP3cft','//t.co/ALepWlrIp7','doodly_official…',\n",
    "'ano','›','الشكر','PHCA','//t.co/gAXaNJZokQ','IN14-21DAYS.HE','//t.co/zqfk8IBNrs','4/28','//t.co/5ntCB6YpNh','tipo','//t.co/GmZarlv7Bw','CHI','//t.co/EAv5ShTtJ5','//t.co/9LGAqM0ePB',\n",
    "'DianaAtwine','FinTech…','LK-05','Maths','//t.co/lR4NTT6ROc','//t.co/svrDN8Fj1e','CSTE','セブン','//t.co/Pl8JYGVZne','1.17','台湾、マスク購入を実名制に','الانبار',\n",
    "'//t.co/DCTC53Z5wm','//t.co/J0px5qKbFe','ABA','//t.co/QymBvyioCe','//t.co/eYTuhbMUkg','//t.co/HWR6SW0ebL','//t.co/zC8C0FXFPQ','//t.co/e8lSxmERyZ','المختبرية',\n",
    "'2/Any','//t.co/jbbsc2A3LH','//t.co/xInOfPUmgr','//t.co/bmm28imSVT','👨\\u200d💻','748-0637','//t.co/jroE1Nr8Ny','on.','n','Spotify','🦕','//t.co/Z34HFPVSDv',\n",
    "'//t.co/bHHS531LhP','今日から職場は全員マスク着用で業務に当たってくださいとのこと。','inane','//t.co/PIMVvPoQfz','//t.co/W0I6OBLRf4',\"y'all\",'العراق','كل','V-Synth','//t.co/lEBfulItxc',\n",
    "'تعلن','//t.co/nK2eTJbZtv','//t.co/w2H0b2pJb8','//t.co/zHa1MDZO93','//t.co/4MBbM5oZhH','sarà','//t.co/N4a3MGBDlX','nicky_2901','//t.co/tnlNG7cvvW','11am',\n",
    "'//t.co/WFjCD7ZEDd','//t.co/TWaBhu9mR4','1,211','//t.co/auB0pVeG7d','//t.co/IdwQ27DdpF','педагог','West','//t.co/YK0qfYUa4R','lorenzozaffiri',\n",
    "\"'leak\",'MOBILE】奥さん、子供たちは休校だってね・・じゃぁ奥さんも休家事しちゃいなよ朝活','29/06/2020','//t.co/SClSErn2cj','//t.co/jEz3uc50B2','//t.co/642K4QjWec','psb_dc','//t.co/6DkDQapDkn',\n",
    "'//t.co/JcFbvMQuNd','SIG','في','محافظة','Phx','dr_ksa12','1:00','//t.co/VecEZfbLma',\"'event\",'//t.co/qaZ2Zw31jE','//t.co/GVCAyNwcTM','//t.co/xgEEtWzIo4',\n",
    "'💜🌺🌼🌻🌸','OBD','ユニリーバやコカ・コーラ、スターバックスなどが続々と、大手SNSのヘイトスピーチ対策が不十分だとして広告掲載を当面見合わせ','تماس','OzCybers','seu','+1','//t.co/03kbvwgYSz','//t.co/e8Etr9nFSw',\n",
    "'//t.co/nRjwRSsmSE','//t.co/LwLqTB0XBI','ny','Ltd','7GTech','18.83','//t.co/rQZphVCP9i','//t.co/AzVDIvUhUE','VS''kwts','//t.co/oYSJtD2Enb','Steph','VideoGamesPlus_','Ayjchan','//t.co/O9htYedxB7',\n",
    "'10.13','//t.co/ylKXRpYeg4','//t.co/pYnxS9x4a3','//t.co/wAr5TTayEC','//t.co/uZHhHbQ47W','//t.co/zoNpVPXV0a','//t.co/RrOiQza4Nm','//t.co/GidfgfcXKn','🗓️Deadline',\n",
    "'00:55:38','.As','KarimZeribi','...','//t.co/hta6isohwv','go',\"https\", \"’\", \"'s\", \"''\", \"``\", \"amp\", \"n't\", \"”\", \"“\", \"de\", \"gt\", \"2020\", \"'re\", \"--\", \"IoT\", \"'m\", \"..\", \n",
    "\"–\", \"en\", \"...\", \"‘\",'health','amp','http','u','one','get','see','using','know','thing','triage','like','first','self-triage','find','would',\n",
    "'la','https',\"n't\",'vid','’','u','fucks','us','use','get','one','de','2020',\"”\",\"‘\",'see','\"','`','health','amp','data','technology',\n",
    "  'ai','doc','blockchain','care','telehealth','need','monitoring','hospital','people','fuck','healthcare','mobile',\n",
    "'visit','medicare','via','covid','information','cost','service','telemedicine','say','new','june','must','patient','stay','iot','follow','wearable','digitalhealth',\n",
    "'take','medium','link','call','sign','access','question','cover','healthtech','make','form','yes','free','dont','ml','self','unit',\n",
    "'th','based','web','tool','mental','member','imaging','website','list','update','page','minute','social','without','change','ehr','system','virtual','situation','contact','also','time','day','case','app','electronic','tracing'}\n",
    "\n",
    "stop_3|= add_stopwords3\n",
    "len(stop_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_1 |= add_stopwords1\n",
    "stop_2 |= add_stopwords2\n",
    "stop_3 |= add_stopwords3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_1['punctuations_removed'] = tweets_1['Noun_Extracted_tweets'].apply(lambda y: [item for item in y if item.lower() not in stop_1])\n",
    "tweets_2['punctuations_removed'] = tweets_2['Noun_Extracted_tweets'].apply(lambda y: [item for item in y if item.lower() not in stop_2])\n",
    "tweets_3['punctuations_removed'] = tweets_3['Noun_Extracted_tweets'].apply(lambda y: [item for item in y if item.lower() not in stop_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1 = tweets_1['punctuations_removed'].tolist() \n",
    "tokens_1 = list(chain(*tokens_1))\n",
    "tokens_freq_1 = collections.Counter(tokens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_2 = tweets_2['punctuations_removed'].tolist() \n",
    "tokens_2 = list(chain(*tokens_2))\n",
    "tokens_freq_2 = collections.Counter(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_3 = tweets_3['punctuations_removed'].tolist() \n",
    "tokens_3 = list(chain(*tokens_3))\n",
    "tokens_freq_3 = collections.Counter(tokens_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exchange', 214),\n",
       " ('doctor', 165),\n",
       " ('state', 159),\n",
       " ('prevention', 158),\n",
       " ('life', 144),\n",
       " ('sensor', 142),\n",
       " ('detection', 132),\n",
       " ('team', 130),\n",
       " ('disease', 125),\n",
       " ('provider', 120),\n",
       " ('market', 120),\n",
       " ('industry', 118),\n",
       " ('world', 117),\n",
       " ('population', 116),\n",
       " ('business', 113),\n",
       " ('program', 113),\n",
       " ('company', 106),\n",
       " ('recognition', 105),\n",
       " ('video', 104),\n",
       " ('please', 104),\n",
       " ('news', 104),\n",
       " ('medicine', 102),\n",
       " ('solution', 101),\n",
       " ('device', 100),\n",
       " ('issue', 100),\n",
       " ('study', 98),\n",
       " ('death', 98),\n",
       " ('country', 97),\n",
       " ('database', 97),\n",
       " ('virus', 95)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq_1.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('support', 211),\n",
       " ('application', 208),\n",
       " ('analytics', 197),\n",
       " ('sensor', 179),\n",
       " ('coronavirus', 170),\n",
       " ('home', 166),\n",
       " ('school', 164),\n",
       " ('state', 158),\n",
       " ('management', 155),\n",
       " ('team', 139),\n",
       " ('solution', 138),\n",
       " ('report', 136),\n",
       " ('business', 135),\n",
       " ('database', 135),\n",
       " ('treatment', 134),\n",
       " ('student', 132),\n",
       " ('join', 130),\n",
       " ('news', 127),\n",
       " ('provider', 127),\n",
       " ('job', 124),\n",
       " ('level', 124),\n",
       " ('decision', 122),\n",
       " ('week', 118),\n",
       " ('detection', 118),\n",
       " ('company', 117),\n",
       " ('research', 117),\n",
       " ('intelligence', 116),\n",
       " ('ehealth', 116),\n",
       " ('medicine', 116),\n",
       " ('program', 115)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq_2.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('community', 229),\n",
       " ('mhealth', 205),\n",
       " ('prevention', 185),\n",
       " ('management', 185),\n",
       " ('application', 181),\n",
       " ('record', 169),\n",
       " ('decision', 167),\n",
       " ('business', 166),\n",
       " ('intelligence', 153),\n",
       " ('analytics', 148),\n",
       " ('home', 146),\n",
       " ('vaccine', 146),\n",
       " ('cybersecurity', 145),\n",
       " ('research', 145),\n",
       " ('artificialintelligence', 136),\n",
       " ('risk', 135),\n",
       " ('interoperability', 124),\n",
       " ('market', 122),\n",
       " ('revenue', 121),\n",
       " ('solution', 121),\n",
       " ('industry', 120),\n",
       " ('device', 114),\n",
       " ('world', 113),\n",
       " ('life', 112),\n",
       " ('security', 112),\n",
       " ('machine', 109),\n",
       " ('program', 109),\n",
       " ('state', 108),\n",
       " ('company', 107),\n",
       " ('medicine', 103)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq_3.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_1['Final']=tweets_1['punctuations_removed'].apply(lambda x:[word for word in x if len(word)>1])\n",
    "tweets_2['Final']=tweets_2['punctuations_removed'].apply(lambda x:[word for word in x if len(word)>1])\n",
    "tweets_3['Final']=tweets_3['punctuations_removed'].apply(lambda x:[word for word in x if len(word)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required column to for K-means birt for phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>markov chain generate text generate text sense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>everytime post video lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hasanshahbaz twitter ban account enter number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow liga recap player mention skrebnev dula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>watch mde month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>6694</td>\n",
       "      <td>httpstcofypewuizg theofficialacm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>6695</td>\n",
       "      <td>sciartexchange facebook garment environment fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>6696</td>\n",
       "      <td>linkedin httpstcoofvymajsym wearabletech weara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>6697</td>\n",
       "      <td>asu center kind research center ecosystem life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>6698</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6699 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                new\n",
       "0         0  markov chain generate text generate text sense...\n",
       "1         1                          everytime post video lmao\n",
       "2         2  hasanshahbaz twitter ban account enter number ...\n",
       "3         3  moscow liga recap player mention skrebnev dula...\n",
       "4         4                                    watch mde month\n",
       "...     ...                                                ...\n",
       "6694   6694                   httpstcofypewuizg theofficialacm\n",
       "6695   6695  sciartexchange facebook garment environment fr...\n",
       "6696   6696  linkedin httpstcoofvymajsym wearabletech weara...\n",
       "6697   6697  asu center kind research center ecosystem life...\n",
       "6698   6698                                                nan\n",
       "\n",
       "[6699 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_1['new'] = [\" \".join(map(str, l)) for l in tweets_1['Final']]\n",
    "tweets_1\n",
    "tweets_1['new'] .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>geoffcmason greencpa lisaformaine strategema s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>night user im root pwn htb hackthebox challang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>employee home look beef defense worker team go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>itox imo iiot marketing digitaltransformation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>twitter expert smartnews httpstcoyrtwneki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>6918</td>\n",
       "      <td>fahrni window desktop io aggregator io</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>6919</td>\n",
       "      <td>particle effect stream rip bitrate httpstcoonv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>6920</td>\n",
       "      <td>tour builder text photo video map create exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>6921</td>\n",
       "      <td>interact business gpn opportunity discus green...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>6922</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                new\n",
       "0         0  geoffcmason greencpa lisaformaine strategema s...\n",
       "1         1  night user im root pwn htb hackthebox challang...\n",
       "2         2  employee home look beef defense worker team go...\n",
       "3         3  itox imo iiot marketing digitaltransformation ...\n",
       "4         4          twitter expert smartnews httpstcoyrtwneki\n",
       "...     ...                                                ...\n",
       "6918   6918             fahrni window desktop io aggregator io\n",
       "6919   6919  particle effect stream rip bitrate httpstcoonv...\n",
       "6920   6920  tour builder text photo video map create exper...\n",
       "6921   6921  interact business gpn opportunity discus green...\n",
       "6922   6922                                                nan\n",
       "\n",
       "[6923 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_2['new'] = [\" \".join(map(str, l)) for l in tweets_2['Final']]\n",
    "tweets_2\n",
    "tweets_2['new'] .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hashgraphguide hedera effect type parallel cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>threddyrex programmer replacement validator cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>linorulli catholicguyshow doug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>online therapy ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>6142</td>\n",
       "      <td>platform capability phase watch danfoss smarts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>6143</td>\n",
       "      <td>education industry format trend impact educati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>6144</td>\n",
       "      <td>paper spreadsheet charity membership manager s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>6145</td>\n",
       "      <td>sariazout newsletter podcasts life twitter col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>6146</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                new\n",
       "0         0  hashgraphguide hedera effect type parallel cha...\n",
       "1         1  threddyrex programmer replacement validator cl...\n",
       "2         2                     linorulli catholicguyshow doug\n",
       "3         3                                           everyone\n",
       "4         4                                  online therapy ad\n",
       "...     ...                                                ...\n",
       "6142   6142  platform capability phase watch danfoss smarts...\n",
       "6143   6143  education industry format trend impact educati...\n",
       "6144   6144  paper spreadsheet charity membership manager s...\n",
       "6145   6145  sariazout newsletter podcasts life twitter col...\n",
       "6146   6146                                                nan\n",
       "\n",
       "[6147 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_3['new'] = [\" \".join(map(str, l)) for l in tweets_3['Final']]\n",
    "tweets_3\n",
    "tweets_3['new'] .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents1 = tweets_1['new'].values\n",
    "documents2 = tweets_2['new'].values\n",
    "documents3 = tweets_3['new'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "encoded_data1 = model.encode(documents1)\n",
    "encoded_data2 = model.encode(documents2)\n",
    "encoded_data3 = model.encode(documents3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=22)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = 5, random_state = 22) # Using 5 clusters here\n",
    "km.fit(encoded_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to return topic per document \n",
    "def doc_topic_id(all_training, km, num_clusters):\n",
    "    clusters = km.labels_.tolist()\n",
    "    docs = {'document_text':all_training, 'topic_id':clusters}\n",
    "    frame = pd.DataFrame(docs, index = [clusters])\n",
    "    for cluster in range(0, num_clusters):\n",
    "        this_cluster_text = \\\n",
    "        frame[frame['topic_id'] == cluster]\n",
    "        all_text = \\\n",
    "        \" \".join(this_cluster_text['document_text'].astype(str))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>markov chain generate text generate text sense...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>everytime post video lmao</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasanshahbaz twitter ban account enter number ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moscow liga recap player mention skrebnev dula...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch mde month</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>httpstcofypewuizg theofficialacm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>sciartexchange facebook garment environment fr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>linkedin httpstcoofvymajsym wearabletech weara...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>asu center kind research center ecosystem life...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>nan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6699 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          document_text  topic_id\n",
       "new                                                              \n",
       "0     markov chain generate text generate text sense...         4\n",
       "1                             everytime post video lmao         4\n",
       "2     hasanshahbaz twitter ban account enter number ...         3\n",
       "3     moscow liga recap player mention skrebnev dula...         4\n",
       "4                                       watch mde month         2\n",
       "...                                                 ...       ...\n",
       "6694                   httpstcofypewuizg theofficialacm         1\n",
       "6695  sciartexchange facebook garment environment fr...         3\n",
       "6696  linkedin httpstcoofvymajsym wearabletech weara...         1\n",
       "6697  asu center kind research center ecosystem life...         1\n",
       "6698                                                nan         2\n",
       "\n",
       "[6699 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_1['new'] = tweets_1['new'].index # Pull out index values into column\n",
    "\n",
    "topics_bert1 = doc_topic_id(documents1, km, 5) # Create dataframe\n",
    "topics_bert1 = topics_bert1.set_index(tweets_1['new']) # Assign doc_id from emails\n",
    "topics_bert1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_bert1 = topics_bert1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(death, 93)</td>\n",
       "      <td>(exchange, 147)</td>\n",
       "      <td>(recognition, 22)</td>\n",
       "      <td>(doctor, 99)</td>\n",
       "      <td>(sensor, 38)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(life, 42)</td>\n",
       "      <td>(market, 56)</td>\n",
       "      <td>(issue, 12)</td>\n",
       "      <td>(team, 77)</td>\n",
       "      <td>(prevention, 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(state, 42)</td>\n",
       "      <td>(provider, 56)</td>\n",
       "      <td>(someone, 10)</td>\n",
       "      <td>(business, 76)</td>\n",
       "      <td>(country, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(virus, 36)</td>\n",
       "      <td>(industry, 47)</td>\n",
       "      <td>(sensor, 10)</td>\n",
       "      <td>(medicine, 73)</td>\n",
       "      <td>(state, 31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(nothing, 35)</td>\n",
       "      <td>(revenue, 42)</td>\n",
       "      <td>(point, 9)</td>\n",
       "      <td>(cancer, 68)</td>\n",
       "      <td>(please, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(lack, 30)</td>\n",
       "      <td>(disease, 37)</td>\n",
       "      <td>(person, 9)</td>\n",
       "      <td>(research, 66)</td>\n",
       "      <td>(number, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(world, 29)</td>\n",
       "      <td>(intelligence, 36)</td>\n",
       "      <td>(number, 8)</td>\n",
       "      <td>(video, 66)</td>\n",
       "      <td>(decision, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(rate, 29)</td>\n",
       "      <td>(solution, 36)</td>\n",
       "      <td>(lot, 8)</td>\n",
       "      <td>(sensor, 64)</td>\n",
       "      <td>(exchange, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(infection, 28)</td>\n",
       "      <td>(hie, 36)</td>\n",
       "      <td>(please, 8)</td>\n",
       "      <td>(prevention, 63)</td>\n",
       "      <td>(team, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(country, 27)</td>\n",
       "      <td>(platform, 35)</td>\n",
       "      <td>(decision, 8)</td>\n",
       "      <td>(news, 62)</td>\n",
       "      <td>(population, 28)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           topic_1             topic_2            topic_3           topic_4  \\\n",
       "0      (death, 93)     (exchange, 147)  (recognition, 22)      (doctor, 99)   \n",
       "1       (life, 42)        (market, 56)        (issue, 12)        (team, 77)   \n",
       "2      (state, 42)      (provider, 56)      (someone, 10)    (business, 76)   \n",
       "3      (virus, 36)      (industry, 47)       (sensor, 10)    (medicine, 73)   \n",
       "4    (nothing, 35)       (revenue, 42)         (point, 9)      (cancer, 68)   \n",
       "5       (lack, 30)       (disease, 37)        (person, 9)    (research, 66)   \n",
       "6      (world, 29)  (intelligence, 36)        (number, 8)       (video, 66)   \n",
       "7       (rate, 29)      (solution, 36)           (lot, 8)      (sensor, 64)   \n",
       "8  (infection, 28)           (hie, 36)        (please, 8)  (prevention, 63)   \n",
       "9    (country, 27)      (platform, 35)      (decision, 8)        (news, 62)   \n",
       "\n",
       "            topic_5  \n",
       "0      (sensor, 38)  \n",
       "1  (prevention, 36)  \n",
       "2     (country, 32)  \n",
       "3       (state, 31)  \n",
       "4      (please, 30)  \n",
       "5      (number, 29)  \n",
       "6    (decision, 29)  \n",
       "7    (exchange, 29)  \n",
       "8        (team, 28)  \n",
       "9  (population, 28)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by topic and get most frequent 10 words per topic\n",
    "\n",
    "topics_bert_grouped1 = topics_bert1.groupby(['topic_id'])['document_text'].apply(','.join).reset_index()\n",
    "topics_bert_grouped1['tokens'] = topics_bert_grouped1['document_text'].apply(re_tokenizer.tokenize)\n",
    "\n",
    "topics_bert_words1 = []\n",
    "for i in topics_bert_grouped1['tokens']:\n",
    "    counter = Counter(i)\n",
    "    topics_bert_words1.append(counter.most_common(10))\n",
    "\n",
    "topics_bert_words1 = pd.DataFrame(topics_bert_words1)\n",
    "topics_bert_words1 = topics_bert_words1.transpose()\n",
    "topics_bert_words1.columns = ['topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5']\n",
    "\n",
    "topics_bert_words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=22)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = 5, random_state = 22) # Using 5 clusters here\n",
    "km.fit(encoded_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geoffcmason greencpa lisaformaine strategema s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>night user im root pwn htb hackthebox challang...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>employee home look beef defense worker team go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itox imo iiot marketing digitaltransformation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter expert smartnews httpstcoyrtwneki</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>fahrni window desktop io aggregator io</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>particle effect stream rip bitrate httpstcoonv...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>tour builder text photo video map create exper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>interact business gpn opportunity discus green...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          document_text  topic_id\n",
       "new                                                              \n",
       "0     geoffcmason greencpa lisaformaine strategema s...         1\n",
       "1     night user im root pwn htb hackthebox challang...         4\n",
       "2     employee home look beef defense worker team go...         1\n",
       "3     itox imo iiot marketing digitaltransformation ...         1\n",
       "4             twitter expert smartnews httpstcoyrtwneki         4\n",
       "...                                                 ...       ...\n",
       "6918             fahrni window desktop io aggregator io         4\n",
       "6919  particle effect stream rip bitrate httpstcoonv...         4\n",
       "6920  tour builder text photo video map create exper...         1\n",
       "6921  interact business gpn opportunity discus green...         1\n",
       "6922                                                nan         0\n",
       "\n",
       "[6923 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_2['new'] = tweets_2['new'].index # Pull out index values into column\n",
    "\n",
    "topics_bert2 = doc_topic_id(documents2, km, 5) # Create dataframe\n",
    "topics_bert2 = topics_bert2.set_index(tweets_2['new']) # Assign doc_id from emails\n",
    "topics_bert2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_bert2 = topics_bert2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(station, 12)</td>\n",
       "      <td>(coronavirus, 117)</td>\n",
       "      <td>(death, 101)</td>\n",
       "      <td>(sensor, 63)</td>\n",
       "      <td>(analytics, 117)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(support, 9)</td>\n",
       "      <td>(school, 106)</td>\n",
       "      <td>(coronavirus, 44)</td>\n",
       "      <td>(capacity, 45)</td>\n",
       "      <td>(management, 75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(person, 9)</td>\n",
       "      <td>(student, 101)</td>\n",
       "      <td>(mask, 40)</td>\n",
       "      <td>(support, 42)</td>\n",
       "      <td>(application, 70)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(sensor, 9)</td>\n",
       "      <td>(medicine, 95)</td>\n",
       "      <td>(nothing, 36)</td>\n",
       "      <td>(decision, 37)</td>\n",
       "      <td>(solution, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(someone, 8)</td>\n",
       "      <td>(application, 89)</td>\n",
       "      <td>(emergency, 36)</td>\n",
       "      <td>(part, 31)</td>\n",
       "      <td>(population, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(let, 8)</td>\n",
       "      <td>(home, 86)</td>\n",
       "      <td>(home, 35)</td>\n",
       "      <td>(number, 30)</td>\n",
       "      <td>(market, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(algorithm, 8)</td>\n",
       "      <td>(job, 85)</td>\n",
       "      <td>(news, 34)</td>\n",
       "      <td>(state, 29)</td>\n",
       "      <td>(provider, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(decision, 7)</td>\n",
       "      <td>(join, 84)</td>\n",
       "      <td>(trump, 33)</td>\n",
       "      <td>(level, 28)</td>\n",
       "      <td>(support, 62)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(recognition, 7)</td>\n",
       "      <td>(team, 80)</td>\n",
       "      <td>(report, 32)</td>\n",
       "      <td>(database, 26)</td>\n",
       "      <td>(intelligence, 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(fact, 7)</td>\n",
       "      <td>(support, 80)</td>\n",
       "      <td>(problem, 31)</td>\n",
       "      <td>(person, 25)</td>\n",
       "      <td>(platform, 52)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            topic_1             topic_2            topic_3         topic_4  \\\n",
       "0     (station, 12)  (coronavirus, 117)       (death, 101)    (sensor, 63)   \n",
       "1      (support, 9)       (school, 106)  (coronavirus, 44)  (capacity, 45)   \n",
       "2       (person, 9)      (student, 101)         (mask, 40)   (support, 42)   \n",
       "3       (sensor, 9)      (medicine, 95)      (nothing, 36)  (decision, 37)   \n",
       "4      (someone, 8)   (application, 89)    (emergency, 36)      (part, 31)   \n",
       "5          (let, 8)          (home, 86)         (home, 35)    (number, 30)   \n",
       "6    (algorithm, 8)           (job, 85)         (news, 34)     (state, 29)   \n",
       "7     (decision, 7)          (join, 84)        (trump, 33)     (level, 28)   \n",
       "8  (recognition, 7)          (team, 80)       (report, 32)  (database, 26)   \n",
       "9         (fact, 7)       (support, 80)      (problem, 31)    (person, 25)   \n",
       "\n",
       "              topic_5  \n",
       "0    (analytics, 117)  \n",
       "1    (management, 75)  \n",
       "2   (application, 70)  \n",
       "3      (solution, 69)  \n",
       "4    (population, 66)  \n",
       "5        (market, 66)  \n",
       "6      (provider, 64)  \n",
       "7       (support, 62)  \n",
       "8  (intelligence, 58)  \n",
       "9      (platform, 52)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by topic and get most frequent 10 words per topic\n",
    "\n",
    "topics_bert_grouped2 = topics_bert2.groupby(['topic_id'])['document_text'].apply(','.join).reset_index()\n",
    "topics_bert_grouped2['tokens'] = topics_bert_grouped2['document_text'].apply(re_tokenizer.tokenize)\n",
    "\n",
    "topics_bert_words2 = []\n",
    "for i in topics_bert_grouped2['tokens']:\n",
    "    counter = Counter(i)\n",
    "    topics_bert_words2.append(counter.most_common(10))\n",
    "\n",
    "topics_bert_words2 = pd.DataFrame(topics_bert_words2)\n",
    "topics_bert_words2 = topics_bert_words2.transpose()\n",
    "topics_bert_words2.columns = ['topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5']\n",
    "\n",
    "topics_bert_words2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=22)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = 5, random_state = 22) # Using 5 clusters here\n",
    "km.fit(encoded_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hashgraphguide hedera effect type parallel cha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>threddyrex programmer replacement validator cl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linorulli catholicguyshow doug</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everyone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online therapy ad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>platform capability phase watch danfoss smarts...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>education industry format trend impact educati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>paper spreadsheet charity membership manager s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>sariazout newsletter podcasts life twitter col...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          document_text  topic_id\n",
       "new                                                              \n",
       "0     hashgraphguide hedera effect type parallel cha...         2\n",
       "1     threddyrex programmer replacement validator cl...         4\n",
       "2                        linorulli catholicguyshow doug         2\n",
       "3                                              everyone         0\n",
       "4                                     online therapy ad         1\n",
       "...                                                 ...       ...\n",
       "6142  platform capability phase watch danfoss smarts...         2\n",
       "6143  education industry format trend impact educati...         1\n",
       "6144  paper spreadsheet charity membership manager s...         4\n",
       "6145  sariazout newsletter podcasts life twitter col...         4\n",
       "6146                                                nan         0\n",
       "\n",
       "[6147 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_3['new'] = tweets_3['new'].index # Pull out index values into column\n",
    "\n",
    "topics_bert3 = doc_topic_id(documents3, km, 5) # Create dataframe\n",
    "topics_bert3 = topics_bert3.set_index(tweets_3['new']) # Assign doc_id from emails\n",
    "topics_bert3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_bert3 = topics_bert3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(decision, 28)</td>\n",
       "      <td>(mhealth, 100)</td>\n",
       "      <td>(risk, 46)</td>\n",
       "      <td>(prevention, 56)</td>\n",
       "      <td>(community, 100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(something, 14)</td>\n",
       "      <td>(management, 95)</td>\n",
       "      <td>(decision, 44)</td>\n",
       "      <td>(death, 50)</td>\n",
       "      <td>(business, 94)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(station, 12)</td>\n",
       "      <td>(cybersecurity, 82)</td>\n",
       "      <td>(interoperability, 39)</td>\n",
       "      <td>(decision, 45)</td>\n",
       "      <td>(research, 87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(recognition, 10)</td>\n",
       "      <td>(analytics, 80)</td>\n",
       "      <td>(prevention, 36)</td>\n",
       "      <td>(risk, 38)</td>\n",
       "      <td>(medicine, 84)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(intelligence, 9)</td>\n",
       "      <td>(application, 78)</td>\n",
       "      <td>(record, 35)</td>\n",
       "      <td>(record, 34)</td>\n",
       "      <td>(student, 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(let, 8)</td>\n",
       "      <td>(community, 78)</td>\n",
       "      <td>(issue, 28)</td>\n",
       "      <td>(problem, 31)</td>\n",
       "      <td>(science, 77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(feel, 8)</td>\n",
       "      <td>(revenue, 73)</td>\n",
       "      <td>(level, 28)</td>\n",
       "      <td>(disease, 31)</td>\n",
       "      <td>(artificialintelligence, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(tweet, 7)</td>\n",
       "      <td>(solution, 71)</td>\n",
       "      <td>(something, 27)</td>\n",
       "      <td>(country, 29)</td>\n",
       "      <td>(vaccine, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(please, 6)</td>\n",
       "      <td>(intelligence, 67)</td>\n",
       "      <td>(home, 26)</td>\n",
       "      <td>(doctor, 27)</td>\n",
       "      <td>(mhealth, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(future, 6)</td>\n",
       "      <td>(provider, 64)</td>\n",
       "      <td>(life, 26)</td>\n",
       "      <td>(issue, 27)</td>\n",
       "      <td>(application, 63)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1              topic_2                 topic_3  \\\n",
       "0     (decision, 28)       (mhealth, 100)              (risk, 46)   \n",
       "1    (something, 14)     (management, 95)          (decision, 44)   \n",
       "2      (station, 12)  (cybersecurity, 82)  (interoperability, 39)   \n",
       "3  (recognition, 10)      (analytics, 80)        (prevention, 36)   \n",
       "4  (intelligence, 9)    (application, 78)            (record, 35)   \n",
       "5           (let, 8)      (community, 78)             (issue, 28)   \n",
       "6          (feel, 8)        (revenue, 73)             (level, 28)   \n",
       "7         (tweet, 7)       (solution, 71)         (something, 27)   \n",
       "8        (please, 6)   (intelligence, 67)              (home, 26)   \n",
       "9        (future, 6)       (provider, 64)              (life, 26)   \n",
       "\n",
       "            topic_4                       topic_5  \n",
       "0  (prevention, 56)              (community, 100)  \n",
       "1       (death, 50)                (business, 94)  \n",
       "2    (decision, 45)                (research, 87)  \n",
       "3        (risk, 38)                (medicine, 84)  \n",
       "4      (record, 34)                 (student, 78)  \n",
       "5     (problem, 31)                 (science, 77)  \n",
       "6     (disease, 31)  (artificialintelligence, 72)  \n",
       "7     (country, 29)                 (vaccine, 71)  \n",
       "8      (doctor, 27)                 (mhealth, 68)  \n",
       "9       (issue, 27)             (application, 63)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by topic and get most frequent 10 words per topic\n",
    "\n",
    "topics_bert_grouped3 = topics_bert3.groupby(['topic_id'])['document_text'].apply(','.join).reset_index()\n",
    "topics_bert_grouped3['tokens'] = topics_bert_grouped3['document_text'].apply(re_tokenizer.tokenize)\n",
    "\n",
    "topics_bert_words3 = []\n",
    "for i in topics_bert_grouped3['tokens']:\n",
    "    counter = Counter(i)\n",
    "    topics_bert_words3.append(counter.most_common(10))\n",
    "\n",
    "topics_bert_words3 = pd.DataFrame(topics_bert_words3)\n",
    "topics_bert_words3 = topics_bert_words3.transpose()\n",
    "topics_bert_words3.columns = ['topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5']\n",
    "\n",
    "topics_bert_words3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
